{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ef336c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-16T21:56:32.385005Z",
     "iopub.status.busy": "2024-09-16T21:56:32.384419Z",
     "iopub.status.idle": "2024-09-16T21:56:33.248001Z",
     "shell.execute_reply": "2024-09-16T21:56:33.246866Z"
    },
    "papermill": {
     "duration": 0.871391,
     "end_time": "2024-09-16T21:56:33.250940",
     "exception": false,
     "start_time": "2024-09-16T21:56:32.379549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ce5b0",
   "metadata": {
    "papermill": {
     "duration": 0.002168,
     "end_time": "2024-09-16T21:56:33.255755",
     "exception": false,
     "start_time": "2024-09-16T21:56:33.253587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Natural Language Processing (NLP): An Introduction\n",
    "\n",
    "In today’s data-driven world, **Natural Language Processing (NLP)** plays a critical role in enabling machines to understand and interact with human language. Whether it's virtual assistants, search engines, or chatbots, NLP is behind many of the tools we use daily. In this article, we'll explore the basics of NLP, why it’s important, and how you can get started with practical Python code examples.\n",
    "\n",
    "### What is Natural Language Processing (NLP)?\n",
    "\n",
    "NLP is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal is to read, decipher, understand, and make sense of the human languages in a valuable way. Applications of NLP are widespread, from **sentiment analysis**, **language translation**, and **text classification**, to **chatbot development**.\n",
    "\n",
    "### Key Concepts in NLP\n",
    "\n",
    "1. **Tokenization**: Breaking text into smaller chunks, such as words or sentences.\n",
    "2. **Stemming/Lemmatization**: Reducing words to their base or root form.\n",
    "3. **Stopwords**: Removing common words (like \"is,\" \"and,\" \"the\") that don’t add much meaning.\n",
    "4. **Bag of Words**: Representing text data in a format that models the frequency of words.\n",
    "\n",
    "Let’s dive into how these concepts work using Python’s popular NLP library, **NLTK**.\n",
    "\n",
    "### Getting Started with NLP in Python\n",
    "\n",
    "We'll begin by installing the necessary libraries:\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "Once installed, let’s go over some basic operations.\n",
    "\n",
    "### 1. **Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting a piece of text into individual words or sentences. It’s one of the first steps in processing raw text data.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download necessary NLTK data files\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"NLP is amazing. It helps computers understand human language!\"\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Words:\", words)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Sentences: ['NLP is amazing.', 'It helps computers understand human language!']\n",
    "Words: ['NLP', 'is', 'amazing', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '!']\n",
    "```\n",
    "\n",
    "### 2. **Removing Stopwords**\n",
    "\n",
    "Stopwords are common words that usually don’t carry significant meaning and can be removed from the text to focus on more important words.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Filtered Words: ['NLP', 'amazing', '.', 'helps', 'computers', 'understand', 'human', 'language', '!']\n",
    "```\n",
    "\n",
    "### 3. **Stemming and Lemmatization**\n",
    "\n",
    "Stemming and lemmatization are techniques used to reduce words to their base forms. **Stemming** removes suffixes (e.g., \"ing,\" \"ed\"), while **lemmatization** transforms a word into its root form based on context.\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Stemming Example\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Lemmatization Example\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Stemmed Words: ['NLP', 'amaz', '.', 'help', 'comput', 'understand', 'human', 'languag', '!']\n",
    "Lemmatized Words: ['NLP', 'amazing', '.', 'help', 'computer', 'understand', 'human', 'language', '!']\n",
    "```\n",
    "\n",
    "### 4. **Bag of Words**\n",
    "\n",
    "A **Bag of Words** is a simple way of representing text data where we count the occurrence of each word in a document. It’s a fundamental technique used in text classification tasks.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"NLP is fun.\", \"I enjoy studying NLP.\", \"NLP is a part of AI.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform documents into Bag of Words representation\n",
    "bow = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Matrix:\\n\", bow.toarray())\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Vocabulary: ['ai', 'enjoy', 'fun', 'is', 'nlp', 'of', 'part', 'studying']\n",
    "Bag of Words Matrix:\n",
    " [[0 0 1 1 1 0 0 0]\n",
    "  [0 1 0 0 1 0 0 1]\n",
    "  [1 0 0 1 1 1 1 0]]\n",
    "```\n",
    "\n",
    "### Why NLP is Important for the Future\n",
    "\n",
    "NLP is at the forefront of many innovations today. With the explosion of unstructured data like emails, social media posts, and reviews, it’s vital to have tools to analyze and interpret human language. As AI becomes more integrated into our daily lives, NLP will continue to drive improvements in human-computer interaction.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "NLP is an exciting and fast-growing field. By understanding the basics, such as tokenization, stemming, and Bag of Words, you can start working on your own text analysis projects. The Python examples above provide a simple introduction, but there are plenty of more advanced topics to explore, including **Named Entity Recognition (NER)**, **Sentiment Analysis**, and **Text Summarization**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd331f",
   "metadata": {
    "papermill": {
     "duration": 0.001922,
     "end_time": "2024-09-16T21:56:33.259864",
     "exception": false,
     "start_time": "2024-09-16T21:56:33.257942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.719104,
   "end_time": "2024-09-16T21:56:33.783769",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-16T21:56:29.064665",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
